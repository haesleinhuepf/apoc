{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce720e69",
   "metadata": {},
   "source": [
    "# Pixel classification on OpenCL-compatible GPUs\n",
    "APOC is based on [pyclesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) and [sklearn](https://scikit-learn.org/stable/).\n",
    "\n",
    "Often, we want to include information from multiple channels (i.e., from different stainings) to be included in the prediction. APOC allows you to do just that. Let's start with loading an example image and some ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030613f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread, imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import apoc\n",
    "\n",
    "image = imread('blobs.tif')\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b081e7d-b5ec-4e1b-9ce7-b10f566b7616",
   "metadata": {},
   "source": [
    "You can now create some annotations.\n",
    "\n",
    "*Note*: Even though your data may be multichannel data, annotations do not have to be multichannel. E.g., for a `[2, 256, 256]`-shaped image, creating an (scarcely) annotated image of shape `[256, 256]` works for APOC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e4f9b2-3199-439c-9827-db7853a7cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # you can use this to make manual annotations\n",
    "    import napari\n",
    "\n",
    "    # start napari\n",
    "    viewer = napari.Viewer()\n",
    "    napari.run()\n",
    "\n",
    "    # add image\n",
    "    viewer.add_image(image)\n",
    "\n",
    "    # add an empty labels layer and keep it in a variable\n",
    "    labels = np.zeros(image.shape).astype(int)\n",
    "    viewer.add_labels(labels)\n",
    "else:\n",
    "    labels = imread('annotations.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_annotations = labels\n",
    "\n",
    "from skimage.io import imshow\n",
    "imshow(manual_annotations, vmin=0, vmax=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a62f8-537c-44f4-ab68-59b7ed601342",
   "metadata": {},
   "source": [
    "We now convert the input data into an artificial 2-channel data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ea3089-d90a-401d-bdb8-06351d8fbeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_1 = image\n",
    "channel_2 = (1 - image)**2\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2)\n",
    "axes[0].imshow(channel_1)\n",
    "axes[0].set_title('First channel')\n",
    "axes[1].imshow(channel_2)\n",
    "axes[1].set_title('Second channel')\n",
    "\n",
    "multi_channel_image = np.stack([channel_1, channel_2])\n",
    "print('Image data dimensions: ', multi_channel_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c04943-d5e2-4097-8368-bd947cde283b",
   "metadata": {},
   "source": [
    "## Training\n",
    "We now train a PixelClassifier, which is under the hood a [scikit-learn RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). After training, the classifier will be converted to [clij-compatible OpenCL code](https://github.com/clEsperanto/clij-opencl-kernels) and save to disk under a given filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c162a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features: original image, a blurred version and an edge image\n",
    "features = \"original gaussian_blur=2 sobel_of_gaussian_blur=2\"\n",
    "\n",
    "# this is where the model will be saved\n",
    "cl_filename = 'my_model.cl'\n",
    "\n",
    "apoc.erase_classifier(cl_filename)\n",
    "clf = apoc.PixelClassifier(opencl_filename=cl_filename)\n",
    "clf.train(features, manual_annotations, multi_channel_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e231f60",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "The classifier can then be used to classify all pixels in the given image. Starting point is again, the feature stack. Thus, the user must make sure that the same features are used for training and for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = clf.predict(image=multi_channel_image)\n",
    "imshow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b3845-223a-4bc7-b0c6-fa081927384b",
   "metadata": {},
   "source": [
    "If we pass the data to the `clf.predict()` function in a wrong format - for instance, by passing single-channel data to a classifier that was trained on multi-channel data - we receive an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb826ed-46fe-4647-a115-43fee6834781",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clf.predict(image = channel_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f8fdc4-2f75-4dc0-819c-6de2855df101",
   "metadata": {},
   "source": [
    "To avoid this issue, we can use the `clf.info()` function to see which dimensions the classifer expects for the prediction. \n",
    "\n",
    "*Note*: For the classifier to work on new data, **only the first dimension** needs to match. In other words:\n",
    "\n",
    "* A classifer trained on a `[2, 256, 256]` image will work for new data with dimensions `[2, 512, 512]`\n",
    "* A classifer trained on a `[2, 256, 256]` image will **not** work for new data with dimensions `[1, 256, 256]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9551ea03-beb9-4b90-a748-ef15b51158b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ac723-ef32-42a4-8e31-5fe6e2abef23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
